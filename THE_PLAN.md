# Shadenfreude - Project Plan

## 1. Project Overview

Shadenfreude is an interactive tool designed to help users create images reminiscent of Pantone color cards, but with a playful twist. Users will upload a photo, select or define a dominant color from it, and the tool will generate a composite image featuring the chosen color swatch, its details (HEX, CMYK, RGB), a custom name, and the user's photo. The project name "Shadenfreude" is a humorous take on "Pantone."

## 2. Core Features (MVP - Step 1, 2, 3)

*   **Image Upload:** Allow users to upload an image from their local device.
*   **Image Manipulation:**
    *   Display the uploaded image.
    *   Provide tools for users to crop and zoom the image to select the desired portion.
*   **Color Input:**
    *   Allow users to pick a color directly from the uploaded image using an eyedropper/color picker tool, which populates the HEX input.
    *   Allow users to manually input a HEX color code (as an alternative or for refinement after picking).
    *   Allow users to manually input a name for the color.
*   **Image Generation:**
    *   Create a layout similar to the example provided:
        *   Left side: Color swatch based on user-provided HEX.
            *   Display "SHADENFREUDE" (all caps) instead of "PANTONE".
            *   Display a unique sequential ID number (e.g., #000000001) for the color, generated by the backend (this replaces the "4975 C" style ID).
            *   Display the user-provided color name.
            *   Display the color in HEX, CMYK, and RGB formats.
        *   Right side: The user-uploaded (and cropped/zoomed) photo.
    *   Use a distinct, aesthetically pleasing font (different from the example).
    *   Ensure proper padding and alignment for all elements.
*   **Download:** Allow users to download the generated image.

## 3. Future Enhancements

*   **Dominant Color Extraction:**
    *   Automatically identify the dominant color(s) from the uploaded image.
    *   Suggest these colors to the user or automatically select the most prominent one.
*   **AI-Powered Color Naming:** (Re-instated as a future enhancement)
    *   Integrate with an AI service (Azure OpenAI or OpenAI).
    *   Generate a creative and context-aware name for the color based on its HEX value, and potentially the content/emotions of the uploaded image, as an alternative to manual input.
*   **User Accounts & Gallery:** (Optional, long-term)
    *   Allow users to save their creations.
    *   Public gallery of generated images.
*   **Advanced AI Image Analysis:** (Optional, long-term)
    *   Integrate with an AI service (Azure OpenAI or OpenAI) to generate descriptive tags or a brief analysis of the image content/mood, which could be displayed or used for future features.

## 4. Technology Stack

*   **Frontend:**
    *   **Framework:** Next.js (React) - Chosen for its robust features, ease of development, and seamless deployment to Vercel.
    *   **Image Cropping/Zooming:** A library like `react-image-crop` or `cropper.js`.
    *   **Color Picking:** Implemented using HTML Canvas API interaction or a lightweight JavaScript library to select a color from the displayed image.
    *   **Styling:** Tailwind CSS for utility-first styling. Consider using `shadcn/ui` as a component library, which integrates well with Tailwind CSS and can be customized to achieve a Neue Brutalist aesthetic.
*   **Backend/API (for image generation and future AI tasks):**
    *   **Language/Framework:** Python with Flask or FastAPI. Can be deployed as serverless functions on Vercel.
    *   **Image Processing (Generation):** Pillow library for creating the composite image, converting colors, and handling fonts.
    *   **Image Processing (Dominant Color Extraction - Future):** `colorgram.py` or OpenCV.
*   **Deployment:** Vercel.
*   **AI Integration (Future):**
    *   OpenAI API / Azure OpenAI Service.
    *   Python SDKs for these services (`openai` library).

## 5. Development Plan

### Phase 1: Frontend Setup & Image Input (MVP - Step 1 part)

*   **Task 1.1:** Initialize Next.js project.
    *   Setup basic project structure, linting, and formatting tools.
*   **Task 1.2:** Implement Image Upload Component.
    *   Allow users to select an image file.
    *   Display a preview of the uploaded image.
*   **Task 1.3:** Implement Image Cropping/Zooming Component.
    *   Integrate `react-image-crop` or similar library.
    *   Allow users to define a crop area and zoom level on the preview.
    *   Output the cropped image data (e.g., as a data URL or blob).
*   **Task 1.4:** Implement Color Input and Picker.
    *   Text input for HEX color code (with validation), populated by the color picker or manual entry.
    *   Text input for the color name (user-provided).
*   **Task 1.5 (New):** Implement Color Picker Functionality.
    *   Integrate an eyedropper/color picker tool that works on the displayed image preview (original or cropped).
    *   When a color is picked, its HEX value should automatically populate the HEX input field.

### Phase 2: Backend Image Generation API (MVP - Step 3 part)

*   **Task 2.1:** Setup Python Backend (Flask/FastAPI).
    *   Create a basic API endpoint (e.g., `/api/generate-image`).
    *   The backend will be responsible for generating the sequential ID string (e.g., #000000001).
    *   Configure for Vercel serverless deployment (e.g., `vercel.json` an `api/` directory).
*   **Task 2.2:** Implement Color Conversion Logic.
    *   Function to convert HEX to RGB.
    *   Function to convert HEX (or RGB) to CMYK.
*   **Task 2.3:** Implement Image Composition Logic (using Pillow).
    *   Accept cropped image data, HEX color, and user-provided color name as input. The backend will generate the sequential ID string.
    *   Create a new image canvas.
    *   Draw the colored rectangle on the left.
    *   Add text elements: "SHADENFREUDE", user-provided color name, generated sequential ID, HEX, RGB, CMYK values.
        *   Research and select a suitable open-source font. Include font file in the project.
    *   Place the cropped user image on the right.
    *   Ensure correct padding and layout.
*   **Task 2.4:** API Endpoint Implementation.
    *   The `/api/generate-image` endpoint should:
        *   Receive image data (base64 or multipart form), HEX color, and user-provided color name.
        *   Call the image composition logic (which includes ID generation).
        *   Return the generated image (e.g., as base64 string or direct image response).

### Phase 3: Frontend-Backend Integration & UI Refinement (MVP - Step 2 & 3 linking)

*   **Task 3.1:** Connect Frontend to Backend.
    *   When the user submits their inputs (cropped image, HEX, name), the frontend sends a request to the `/api/generate-image` endpoint.
*   **Task 3.2:** Display Generated Image.
    *   Show the image returned by the API to the user.
*   **Task 3.3:** Implement Download Functionality.
    *   Allow users to download the final generated image.
*   **Task 3.4:** UI/UX Polish.
    *   Style the application for a clean and intuitive user experience, aiming for a **Neue Brutalist** aesthetic. Utilize `shadcn/ui` components as a base where appropriate, customized to fit this style.
    *   Add loading states and error handling.

### Phase 4: Deployment to Vercel

*   **Task 4.1:** Prepare for Vercel.
    *   Ensure `requirements.txt` (for Python backend) and `package.json` (for Next.js frontend) are correctly configured.
    *   Create/verify `vercel.json` if custom configurations are needed (e.g., for Python runtime, rewrites).
        ```json
        {
          "version": 2,
          "builds": [
            {
              "src": "frontend/package.json",
              "use": "@vercel/next"
            },
            {
              "src": "api/**/*.py",
              "use": "@vercel/python",
              "config": { "maxLambdaSize": "50mb" } // May need adjustment for Pillow/fonts
            }
          ],
          "routes": [
            {
              "src": "/api/(.*)",
              "dest": "api/$1"
            },
            {
              "handle": "filesystem"
            },
            {
              "src": "/(.*)",
              "dest": "/(.*)" 
            }
          ]
        }
        ```
    *   Font files need to be accessible by the Python backend if rendering server-side. Consider placing them in a directory accessible by the serverless function or packaging them appropriately.
*   **Task 4.2:** Deploy.
    *   Connect GitHub repository to Vercel.
    *   Configure build settings if necessary.
    *   Trigger deployment and test.

### Phase 5: Future - Dominant Color Extraction

*   **Task 5.1:** Research and Select Python Library.
    *   Evaluate `colorgram.py`, `scikit-image`, or OpenCV for dominant color extraction.
*   **Task 5.2:** Implement Backend Logic.
    *   Create a new API endpoint (e.g., `/api/extract-color`).
    *   This endpoint takes the uploaded image and returns a list of dominant HEX colors.
*   **Task 5.3:** Frontend Integration.
    *   Call the new endpoint after image upload.
    *   Display suggested colors to the user, allowing them to pick one, which then populates the HEX input.

### Phase 6: Future - AI Color Naming (Re-instated)

*   **Task 6.1:** Setup AI API Access.
    *   Obtain API keys for Azure OpenAI or OpenAI.
    *   Store keys securely as environment variables on Vercel.
*   **Task 6.2:** Design Prompt for AI.
    *   Craft a prompt that takes the HEX color, and potentially a description/keywords from the image, to generate a creative name.
    *   Example prompt structure: "Generate a short, evocative name for a color with HEX code #{hex_color}. The color is primarily featured in an image depicting {image_description_or_keywords}. The name should evoke {emotion_or_style}."
*   **Task 6.3:** Implement Backend Logic for AI Naming.
    *   Create a new API endpoint (e.g., `/api/generate-name`).
    *   This endpoint takes the HEX color (and optionally image context).
    *   Uses the OpenAI Python SDK to call the completion API.
    *   Parses the AI's response and returns the generated name.
*   **Task 6.4:** Frontend Integration.
    *   Add a button "Generate Name with AI" (or similar UI element).
    *   Call the `/api/generate-name` endpoint.
    *   Populate the color name input field with the AI's suggestion, allowing the user to accept or edit it.

## 6. Vercel Deployment Files and Setup (Original Shadenfreude Plan)

*Note: This section was part of the original Shadenfreude plan. Some details, like paths (`frontend/package.json`), need to be adapted to the merged `sf` project structure where Next.js is at the root and the API is in `api/`.*

*   **`package.json`:** (For Next.js frontend - now at the root of `sf`)
    *   Standard Next.js scripts (`dev`, `build`, `start`, `lint`).
    *   Dependencies: `next`, `react`, `react-dom`, `react-image-crop`, `tailwindcss`.
*   **`requirements.txt`:** (For Python backend in `sf/api/` directory)
    *   `FastAPI`
    *   `Pillow`
    *   `python-dotenv`
    *   `uvicorn` (for local dev)
    *   `slowapi` (for rate limiting)
    *   `openai` (for future AI integration)
*   **`vercel.json`:** (In the root of `sf`)
    *   To configure builds for both Next.js (root) and Python serverless functions (in `api/`).
    *   The `sf/vercel.json` is already quite simple:
        ```json
        {
          "functions": {
            "api/**": {
              "excludeFiles": "{.next,.git,node_modules}/**"
            }
          }
        }
        ```
        This configuration tells Vercel that anything in the `api/` directory that can be a serverless function should be treated as such, and the Python runtime will be inferred. The Next.js part is handled by Vercel automatically when it detects a Next.js project at the root.
*   **Font Files:**
    *   Choose an open-source font. Download `.ttf` or `.otf` files.
    *   Place them in a directory accessible by the Python backend (e.g., `sf/api/assets/fonts/YourFont.ttf`).
    *   The Python code (`get_font` in `sf/api/index.py`) will need to be updated to reference this path explicitly for reliable font loading on Vercel, instead of relying on system fonts.

## 7. Strategy for AI Integration (OpenAI / Azure OpenAI)

This section outlines the strategy for integrating AI services like OpenAI or Azure OpenAI. This approach applies to planned future enhancements such as **AI-Powered Color Naming** (detailed in Phase 6) and **Advanced AI Image Analysis** (see Section 3), as well as any other potential AI-driven features.

*   **API Key Management:**
    *   Store API keys as environment variables on Vercel (e.g., `OPENAI_API_KEY`).
    *   Use `os.environ.get()` in Python to access them.
*   **SDK Usage:**
    *   Install the `openai` Python library (`uv pip install openai`).
    *   Initialize the client with the API key when needed.
*   **Backend API Endpoint(s) for AI tasks:**
    *   Dedicated endpoints (e.g., `/api/generate-color-name`, `/api/analyze-image-content`) would handle requests from the frontend.
*   **Prompt Engineering Example:** (As in original doc)
*   **Error Handling & Fallbacks:** (As in original doc)

## 8. Development Setup & Running Locally (Adapted for `sf` project)

### Prerequisites

*   Node.js (v18 or later recommended)
*   pnpm (or npm/yarn)
*   Python (v3.9 or later recommended)
*   `uv`

### Setup Instructions

1.  **Clone the `sf` Repository (if you haven't already).**

2.  **Set up Python Virtual Environment & Install Dependencies (in `sf` root):**
    ```bash
    uv venv
    source .venv/bin/activate  # macOS/Linux. For Windows: .venv\Scripts\activate
    uv pip install -r requirements.txt
    ```

3.  **Install Frontend Dependencies (in `sf` root):**
    ```bash
    pnpm install 
    ```

### Running the Application Locally (from `sf` root)

1.  **Ensure Python virtual environment is active:**
    ```bash
    source .venv/bin/activate 
    ```
2.  **Start both Frontend and Backend concurrently:**
    ```bash
    pnpm dev
    ```
    *   Next.js frontend will be on `http://localhost:3000`.
    *   FastAPI backend (Uvicorn) will be on `http://localhost:8000` (proxied via `/api/` by Next.js dev server).

3.  **Access the application at `http://localhost:3000` in your browser.** 